# -*- coding: utf-8 -*-
"""Multilinear_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w610b-dCShv4YTLdyayeZVYdkhVi1Ojx
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from geopy.distance import geodesic
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBRegressor

# Funkcja do przekształcenia daty z ułamkowego roku na pełny rok i miesiąc
def convert_fractional_year(fractional_year):
    # Wyodrębnienie pełnego roku
    year = int(fractional_year)

    # Obliczenie części dziesiętnej i przekształcenie na miesiąc
    month_fraction = fractional_year - year
    month = int(round(month_fraction * 12)) + 1

    # Ustalenie pierwszego dnia miesiąca
    return pd.Timestamp(year=year, month=month, day=1)

def plot_predicted_vs_actual_advanced(y_test, y_pred, model_name):
    plt.figure(figsize=(10, 6))

    # Obliczanie błędów
    errors = np.abs(y_test - y_pred)

    # Rysowanie rzeczywistych vs przewidywanych z gradientem kolorów na podstawie błędów
    scatter = plt.scatter(y_test, y_pred, c=errors, cmap='viridis', alpha=0.6, label='Przewidywane', marker='x')

    # Dodanie paska kolorów dla błędów
    cbar = plt.colorbar(scatter)
    cbar.set_label('The size of the measurement error')

    # Rysowanie linii idealnego dopasowania (y = x)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Idealne dopasowanie (y = x)')

    # Tytuł, etykiety osi i legenda
    plt.title(f'Predicted: {model_name}')
    plt.xlabel('Real values')
    plt.ylabel('Predicted values')
    plt.legend()
    plt.grid(True)

    plt.show()

#Wczytanie zbioru danych
df = pd.read_csv('real_estate.csv')
df_original = df.copy()

df.head()

df.rename(columns={
    'Y house price of unit area': 'Price',
    'X1 transaction date': 'Transaction_Date',
    'X2 house age': 'House_Age',
    'X3 distance to the nearest MRT station': 'Distance_to_MRT',
    'X4 number of convenience stores': 'Convenience_Stores',
    'X5 latitude': 'Latitude',
    'X6 longitude': 'Longitude'
    }, inplace=True)

df.describe()

print(df.isnull().sum())

print((df == 0).sum())

# Przekształcenie kolumny 'X1 transaction date' z ułamkowego roku na pełną datę
df['Transaction_Date'] = df['Transaction_Date'].apply(convert_fractional_year)

# Wyodrębnianie roku, miesiąca i dnia tygodnia z przekształconej daty
df['Year'] = df['Transaction_Date'].dt.year
df['Month'] = df['Transaction_Date'].dt.month

df.drop(columns=['Transaction_Date'], inplace=True)

# Sprawdzenie przekształceń
print(df.head())

#rozbicie kolumny year na rok 2012 i 2013
df = pd.get_dummies(df, columns=['Year'], drop_first=False)
df = pd.get_dummies(df, columns=['Month'], drop_first=False)
# Sprawdzenie, jak wyglądają kolumny po przekształceniu
print(df.columns)

# Liczba transakcji w poszczególnych miesiącach dla 2012 i 2013
monthly_transactions_2012 = [df[(df['Year_2012'] == 1) & (df[f'Month_{i}'] == 1)].shape[0] for i in range(1, 13)]
monthly_transactions_2013 = [df[(df['Year_2013'] == 1) & (df[f'Month_{i}'] == 1)].shape[0] for i in range(1, 13)]

# Tworzenie osobnych wykresów dla każdego roku
fig, axes = plt.subplots(1, 2, figsize=(8, 4), sharey=True)

# Wykres dla 2012 roku
axes[0].bar(range(1, 13), monthly_transactions_2012, color='lightcoral', edgecolor='black')
axes[0].set_title("Number of transactions in 2012")
axes[0].set_xlabel("Month")
axes[0].set_ylabel("Number of transactions")
axes[0].set_xticks(range(1, 13))
axes[0].grid(axis='y', linestyle='--', alpha=0.5)

# Wykres dla 2013 roku
axes[1].bar(range(1, 13), monthly_transactions_2013, color='lightblue', edgecolor='black')
axes[1].set_title("Number of transactions in 2013")
axes[1].set_xlabel("Month")
axes[1].set_xticks(range(1, 13))
axes[1].grid(axis='y', linestyle='--', alpha=0.5)

# Liczba transakcji według roku (suma kolumn Year_2012 i Year_2013)
yearly_transactions = {
    "2012": df['Year_2012'].sum(),
    "2013": df['Year_2013'].sum()
}

# Wykres słupkowy liczby transakcji według roku
plt.figure(figsize=(8, 4))
plt.bar(yearly_transactions.keys(), yearly_transactions.values(), color=['lightcoral', 'lightblue'], edgecolor='black')
plt.title("Comparison of the number of transactions between 2012 and 2013")
plt.xlabel("Year")
plt.ylabel("Number of transactions")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# Wyświetlenie wykresów
plt.tight_layout()
plt.show()

# Średnie ceny w poszczególnych miesiącach dla 2012 i 2013
monthly_avg_price_2012 = [df[(df['Year_2012'] == 1) & (df[f'Month_{i}'] == 1)]['Price'].mean() for i in range(1, 13)]
monthly_avg_price_2013 = [df[(df['Year_2013'] == 1) & (df[f'Month_{i}'] == 1)]['Price'].mean() for i in range(1, 13)]

# Wykresy średnich cen w poszczególnych miesiącach dla 2012 i 2013
fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)

# Wykres dla 2012 roku
axes[0].plot(range(1, 13), monthly_avg_price_2012, marker='o', color='lightcoral', linestyle='-', label='2012')
axes[0].set_title("Average Real Estate Prices in 2012")
axes[0].set_xlabel("Month")
axes[0].set_ylabel("Average Price per Unit (TWD Thousand)")
axes[0].set_xticks(range(1, 13))
axes[0].grid(axis='y', linestyle='--', alpha=0.5)

# Wykres dla 2013 roku
axes[1].plot(range(1, 13), monthly_avg_price_2013, marker='o', color='lightblue', linestyle='-', label='2013')
axes[1].set_title("Average Real Estate Prices in 2013")
axes[1].set_xlabel("Month")
axes[1].set_xticks(range(1, 13))
axes[1].grid(axis='y', linestyle='--', alpha=0.5)

# Wyświetlenie wykresów
plt.tight_layout()
plt.show()

# Porównanie średnich cen w latach 2012 i 2013
avg_price_2012 = df[df['Year_2012'] == 1]['Price'].mean()
avg_price_2013 = df[df['Year_2013'] == 1]['Price'].mean()

# Wykres słupkowy porównujący średnie ceny w latach 2012 i 2013
plt.figure(figsize=(8, 6))
plt.bar(['2012', '2013'], [avg_price_2012, avg_price_2013], color=['lightcoral', 'lightblue'], edgecolor='black')
plt.title("Comparison of Average Real Estate Prices in 2012 and 2013")
plt.xlabel("Year")
plt.ylabel("Average Price per Unit (TWD Thousand)")
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

# Sprawdzenie podstawowych statystyk współrzędnych geograficznych
print(df[['Latitude', 'Longitude']].describe())

# Wykres rozrzutu dla współrzędnych geograficznych, aby zobaczyć ich rozmieszczenie


plt.figure(figsize=(8, 4))
plt.scatter(df['Longitude'], df['Latitude'], alpha=0.5)
plt.title("Real estate destinations - Longitude vs Latitude")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.grid(True)
plt.show()

# Obliczanie średniej współrzędnych w celu ustalenia współrzędnych skupiska (centrum)
centroid_latitude = df['Latitude'].mean()
centroid_longitude = df['Longitude'].mean()

# Punkt centralny na podstawie centroidu
centroid_point = (centroid_latitude.round(2), centroid_longitude.round(2))
print(f"Centroid (Mean): {centroid_point}")

# Użycie punktu centralnego z wybranego podejścia
central_point = centroid_point

# Obliczanie odległości od punktu centralnego
df['Distance_from_Center'] = df.apply(lambda row: geodesic((row['Latitude'], row['Longitude']), central_point).km, axis=1)

# Sprawdzenie wyniku
print(df[['Latitude', 'Longitude', 'Distance_from_Center']].head())

df.drop(columns=['Latitude', 'Longitude','No'], inplace=True)

# Wykres rozrzutu: dystans od centrum vs cena nieruchomości
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Distance_from_Center', y='Price', data=df, alpha=0.6, color='blue')

# Dodanie linii trendu
sns.regplot(x='Distance_from_Center', y='Price', data=df, scatter=False, color='red', line_kws={"linestyle":"--"})

# Tytuł, etykiety osi i siatka
plt.title('Average Real Estate Prices Depending on the Distance from the Center')
plt.xlabel('Distance from the Center (km)')
plt.ylabel('Average Price per Unit (TWD Thousand)')
plt.grid(True, linestyle='--', alpha=0.5)

df.corr()

print('Wykresy z wartosciamy odstajacymi')
columns = ['House_Age', 'Distance_to_MRT', 'Convenience_Stores','Distance_from_Center']
sns.set_style("darkgrid")
plt.figure(figsize=(12, 8))
for i, column in enumerate(columns):
    plt.subplot(3, 3, i + 1)  # 4 wiersze, 3 kolumny
    sns.boxplot(y=df[column])
    plt.title(column)
plt.tight_layout()
plt.show()

corr_matrix = df.drop(['Year_2012', 'Year_2013','Month_1','Month_2','Month_3','Month_4','Month_5','Month_6','Month_7','Month_8','Month_9','Month_10','Month_11','Month_12'], axis=1).corr()

sns.heatmap(corr_matrix, annot=True, cmap='Reds')
plt.show()

scaler = StandardScaler()

# Lista cech do skalowania bez zmiennej celu Price
features_to_scale = ['House_Age', 'Distance_to_MRT',
                     'Convenience_Stores', 'Distance_from_Center']

df_scaled = df.copy()

# Zastosowanie StandardScaler tylko do wybranych cech ciągłych
scaler = StandardScaler()
df_scaled[features_to_scale] = scaler.fit_transform(df_scaled[features_to_scale])

# Sprawdzenie wyników skalowania
print(df_scaled[features_to_scale].head())

# Cechy (X) - wszystkie kolumny poza 'target' (nasza zmienna docelowa)
X = df_scaled.drop('Price', axis=1)

# Zmienna docelowa (y) - kolumna target
y = df_scaled['Price']

# Podzial na zbior testowy i treningowy 80/20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"The size of the training set: {X_train.shape}")
print(f"The size of the test set: {X_test.shape}")

# Tworzenie modelu regresji liniowej
model = LinearRegression()

# Trenowanie modelu na zbiorze treningowym
model.fit(X_train, y_train)

# Dokonanie predykcji na zbiorze testowym
y_pred = model.predict(X_test)

# Ocena modelu
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"LinearRegression - R²: {r2:.2f}")
print(f"LinearRegression - Mean absolute error (MAE): {mae:.2f}")
print(f"LinearRegression - Mean root mean square error (RMSE): {rmse:.2f}")

# Tworzenie modelu Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Trenowanie modelu na zbiorze treningowym
rf_model.fit(X_train, y_train)

# Dokonanie predykcji na zbiorze testowym
rf_pred = rf_model.predict(X_test)

# Ocena modelu
rf_r2 = r2_score(y_test, rf_pred)
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))

print(f"Random Forest - R²: {rf_r2:.2f}")
print(f"Random Forest - Mean absolute error (MAE): {rf_mae:.2f}")
print(f"Random Forest - Mean root mean square error (RMSE): {rf_rmse:.2f}")

# Wyciągnięcie ważności cech z modelu Random Forest
feature_importances = rf_model.feature_importances_

# Tworzenie DataFrame do uporządkowania ważności cech
features = X_train.columns
importance_df = pd.DataFrame({'Characteristics': features, 'Validity': feature_importances})
importance_df = importance_df.sort_values(by='Validity', ascending=False)

# Wykres ważności cech
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Characteristics'], importance_df['Validity'], color='skyblue')
plt.xlabel('Validity')
plt.title('Validity characteristics in Random Forest model')
plt.gca().invert_yaxis()  # Odwrócenie osi, aby najważniejsze cechy były na górze
plt.grid(True)
plt.show()

# Wyświetlenie tabeli z ważnością cech
print(importance_df)

# Tworzenie modelu drzewa decyzyjnego
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

# Dokonanie predykcji i ocena modelu
dt_pred = dt_model.predict(X_test)

print(f"Decision Tree - R²: {r2_score(y_test, dt_pred):.2f}")
print(f"Decision Tree - MAE: {mean_absolute_error(y_test, dt_pred):.2f}")
print(f"Decision Tree - RMSE: {np.sqrt(mean_squared_error(y_test, dt_pred)):.2f}")

# Ustalamy zakresy hiperparametrów do przeszukania
param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 10],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'min_samples_split': [2, 5, 10]
}

# Tworzymy model Gradient Boosting
gbm_model = GradientBoostingRegressor(random_state=42)

# Randomized Search z 10-krotną walidacją krzyżową
random_search = RandomizedSearchCV(
    gbm_model, param_grid, n_iter=20, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42
)

# Dopasowanie modelu do danych treningowych
random_search.fit(X_train, y_train)

# Najlepsze parametry
best_params = random_search.best_params_
print(f"Najlepsze parametry: {best_params}")

# Stworzenie modelu z najlepszymi parametrami
best_gbm_model = random_search.best_estimator_

# Dokonanie predykcji na zbiorze testowym
best_gbm_pred = best_gbm_model.predict(X_test)

# Ocena modelu
best_gbm_r2 = r2_score(y_test, best_gbm_pred)
best_gbm_mae = mean_absolute_error(y_test, best_gbm_pred)
best_gbm_rmse = np.sqrt(mean_squared_error(y_test, best_gbm_pred))

print(f"Gradient Boosting (Tuned) - R²: {best_gbm_r2:.2f}")
print(f"Gradient Boosting (Tuned) - MAE: {best_gbm_mae:.2f}")
print(f"Gradient Boosting (Tuned) - RMSE: {best_gbm_rmse:.2f}")

# Tworzenie modelu XGBoost
xgb_model = XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)

# Dokonanie predykcji i ocena modelu
xgb_pred = xgb_model.predict(X_test)

print(f"XGBoost - R²: {r2_score(y_test, xgb_pred):.2f}")
print(f"XGBoost - MAE: {mean_absolute_error(y_test, xgb_pred):.2f}")
print(f"XGBoost - RMSE: {np.sqrt(mean_squared_error(y_test, xgb_pred)):.2f}")

plot_predicted_vs_actual_advanced(y_test, y_pred, "Linear Regression")
plot_predicted_vs_actual_advanced(y_test, rf_pred, "RandomForestRegressor")
plot_predicted_vs_actual_advanced(y_test, dt_pred, "DecisionTreeRegressor")
plot_predicted_vs_actual_advanced(y_test, best_gbm_pred, "GradientBoosting")
plot_predicted_vs_actual_advanced(y_test, xgb_pred, "XGBRegressor")